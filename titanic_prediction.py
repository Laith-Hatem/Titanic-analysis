# -*- coding: utf-8 -*-
"""Titanic-Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gv2idgV4H_L6MXBp3LUo4-2WQPUAOB5B

# Import the Important Libraries
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import StackingClassifier

df = pd.read_csv('titanic.csv')

"""# As we've known from the Analysis, we will do some data cleaning"""

df.drop(['alive','deck','who','class','embark_town'],axis = 1 , inplace=True)
df['age'] = df.groupby(['pclass', 'sex', 'alone'])['age'].transform('median')
df.dropna(axis = 0 , inplace = True)

"""#Data Preparing

We will use Ordinal Encoder type because 'sex' and 'embarked' features had an impact on the survival people
"""

ord_encoder = OrdinalEncoder()
categorical = ['sex','embarked']
df[categorical] = ord_encoder.fit_transform(df[categorical])

df.head(2)

"""We can see that 'sex' and 'embarked' are float type, we've got to transfer them to int type"""

df[['sex','embarked']] = df[['sex','embarked']].astype('int')

df.head(2)

"""#Sepreate the target of the data"""

data = df.drop('survived',axis = 1)
target = df['survived']

"""#Split the data"""

x_train , x_test , y_train , y_test = train_test_split(data,target,test_size = 0.2,random_state = 42)

x_train.shape , y_train.shape , x_test.shape , y_test.shape

"""#Scaling the data for better results"""

scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

"""#We can now train models and evaluate them"""

#Logistic Regression

log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
y_pred = log_reg.predict(x_test)
print(classification_report(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))

"""It is a little bit good, but we need something more powerful

#Random Forrest
"""

rf = RandomForestClassifier()
rf.fit(x_train,y_train)
y_pred = rf.predict(x_test)
print(classification_report(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))

"""What a surprise, Logistic Regression now is better than Random Forrest!

#Let us try with Decision Tree
"""

dr = DecisionTreeClassifier()
dr.fit(x_train,y_train)
y_pred = dr.predict(x_test)
print(classification_report(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))

"""#We will use Stacking (Merge number of models to take the decisions together)"""

base_models = [('lr', LogisticRegression()), ('rf', RandomForestClassifier()), ('dt', DecisionTreeClassifier())]
stacking_model = StackingClassifier(estimators=base_models, cv=5)
stacking_model.fit(x_train, y_train)
y_pred = stacking_model.predict(x_test)
print(classification_report(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))

